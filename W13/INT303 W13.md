# Boosting

对于一个复杂的决策树，它拥有低偏差和高方差。Bagging 的思路是构建多个模型来降低方差。而我们的新的思路是减少简单树的偏差使它们更具表现力 - boosting。

和 bagging 不同，bagging 是最小化方差，boosting 是最小化偏差。

Ensemble methods that minimize bias：

* Functional Gradient Descent
* Boosting
* Adboost

## Boosting Algorithms

Boosting 背后的关键直觉是，人们可以采用简单模型 $\left\{T_{h}\right\}_{h \in H}$ 的集成，并将它们加法地组合成一个更复杂的模型。
$$
T=\sum_{h} \lambda_{h} T_{H}
$$
其中每个模型 $T_h$ 可能不能很好的预测数据 (高偏差)，但集成的线性组合可以是富有表现力/灵活性的。

### GRADIENT BOOSTING

Gradient boosting 是一种通过添加简单模型来迭代构建复杂回归模型 $T$ 的方法。添加到集成中的每个新的简单模型都弥补了当前集成的弱点。

![W13 Gradient boosting](D:\Files\Learning Materials\Y4\INT303-BigDataAnalytics\MD Pictures\W13 Gradient boosting.png)

注：大概过程就是：先训一个模型，然后用该模型对训练集预测，再求出真实值和预测值之间的 residuals ($r_n$)；用 residuals 代替训练集中的真实值，再训练新的模型，最后将模型进行加权相加。

![W13 Gradient boosting 2](D:\Files\Learning Materials\Y4\INT303-BigDataAnalytics\MD Pictures\W13 Gradient boosting 2.png)

### WHY DOES GRADIENT BOOSTING WORK?

每个简单模型 $T^{(i)}$ 都是对集成模型 $T$ 的误差的建模。这样，我们把 $T^{(i)}$ 添加到集成模型 $T$ 中，residual 会减少。
$$
r_{n}-\lambda T^{(i)}\left(x_{n}\right)
$$
现在我们的目的就是让 residual 最小化，这就是一个优化问题，可以使用 gradient descent 来进行。

### GRADIENT BOOSTING AS GRADIENT DESCENT

![W13 GRADIENT BOOSTING AS GRADIENT DESCENT](D:\Files\Learning Materials\Y4\INT303-BigDataAnalytics\MD Pictures\W13 GRADIENT BOOSTING AS GRADIENT DESCENT.png)

注：$\lambda r_n$ 是我们得到的梯度，我们会根据梯度来训练新的模型。

### CHOOSING A LEARNING RATE

在理想条件下，梯度下降以迭代方式近似并收敛到最优。

但是，我们应该在什么时候终止梯度下降：

* 我们可以限制下降过程中的迭代次数。但是，对于最大迭代的任意选择，我们不能保证我们最终足够接近最佳
* 如果在更新足够小时停止下降（例如，T的残差很小），我们会遇到一个新问题：算法可能永远不会终止

这两个问题都与学习速率 λ 的大小有关。

对于恒定学习速率 λ，如果 λ 太小，则需要多次迭代才能达到最优值。如果 λ 太大，算法可能会在最佳值附近"震荡"，并且永远不会足够接近。

如何选择 𝜆:

* 如果 𝜆 是一个常数，那么它应该通过交叉验证来调整

* 对于更好的结果，让 𝜆 成为一个变量，它的值依赖于梯度
  $$
  \lambda=h(\|\nabla f(x)\|)
  $$

  * 其中，$\|\nabla f(x)\|$ 是梯度 $\nabla f(x)$ 的大小。因此在最优值附近时，梯度小，𝜆 也小；离最优值远时，梯度大，𝜆 也大。

## AdaBoost

### COMPONENTS: DECISION STUMPS

AdaBoost 的每棵树都只有一个根节点和两个叶子节点，因此也叫决策树桩 (decision stump)。

对于一个二分类问题 (+1/-1)，在 adaBoost 中会生成一系列的分量预测器 (component classifiers):
$$
h(\mathbf{x};\theta) = sign(w_1x_k - w_0)
$$
其中 $\theta=\{k,w_1,w_0\}$。因为 adaBoost 的每棵树都只有一个根节点和两个叶子节点，因此每个树只能对一个输入向量的一个分量进行预测 (比如下面的胸口疼痛)，$x_k$ 就是其中的一个分量。

![W13 adaboost](D:\Files\Learning Materials\Y4\INT303-BigDataAnalytics\MD Pictures\W13 adaboost.png)

### GRADIENT DESCENT WITH EXPONENTIAL LOSS

我们使用 exponential loss 来计算 adaBoost 的分类误差：
$$
\operatorname{ExpLoss}=\frac{1}{N} \sum_{n=1}^{N} \exp \left(-y_{n} \hat{y}_{n}\right), y_{n} \in\{-1,1\}
$$
![W13 GRADIENT DESCENT WITH EXPONENTIAL LOSS 1](D:\Files\Learning Materials\Y4\INT303-BigDataAnalytics\MD Pictures\W13 GRADIENT DESCENT WITH EXPONENTIAL LOSS 1.png)

![W13 GRADIENT DESCENT WITH EXPONENTIAL LOSS 2](D:\Files\Learning Materials\Y4\INT303-BigDataAnalytics\MD Pictures\W13 GRADIENT DESCENT WITH EXPONENTIAL LOSS 2.png)

和 gradient boosting 一样，我们训练新的决策树桩近似梯度 $\lambda w_n y_n$。

![W13 GRADIENT DESCENT WITH EXPONENTIAL LOSS 3](D:\Files\Learning Materials\Y4\INT303-BigDataAnalytics\MD Pictures\W13 GRADIENT DESCENT WITH EXPONENTIAL LOSS 3.png)

也就是说，我们使用 exponential loss 的梯度下降来迭代训练新的简单模型，而该模型侧重于先前模型错误分类的点。



AdaBoost 中会有很多弱模型，接下来，我们需要定义一个损失函数，以便我们可以确定要添加的新分量 h(x; θ) (adaBoost 会一个一个训练新的分量模型，并将其加入总模型)，以及它占的权重。
$$
h_{m}(\mathbf{x})=\alpha_{1} h\left(\mathbf{x} ; \theta_{1}\right)+\ldots+\alpha_{m} h\left(\mathbf{x} ; \theta_{m}\right)
$$
虽然损失函数有很多选项，但我们在这里只考虑一个简单的指数损失:
$$
exp\{-y \ h_m(\mathbf{x})\}
$$
![W13 GRADIENT DESCENT WITH EXPONENTIAL LOSS 4](D:\Files\Learning Materials\Y4\INT303-BigDataAnalytics\MD Pictures\W13 GRADIENT DESCENT WITH EXPONENTIAL LOSS 4.png)

注：上式就是 $exp\{-y \ h_m(\mathbf{x})\}$ 的展开。

现在我们就要解决两个问题，一个是要添加的新分量 h(x; θ)，另一个是它占的权重 $\alpha$。

我们先将 $\alpha$ 从式子中消除，以方便找到合适的 h(x; θ)。

![W13 GRADIENT DESCENT WITH EXPONENTIAL LOSS 5](D:\Files\Learning Materials\Y4\INT303-BigDataAnalytics\MD Pictures\W13 GRADIENT DESCENT WITH EXPONENTIAL LOSS 5.png)

之后找到能最小化 error 的 h(x; θ)。

![W13 GRADIENT DESCENT WITH EXPONENTIAL LOSS 6](D:\Files\Learning Materials\Y4\INT303-BigDataAnalytics\MD Pictures\W13 GRADIENT DESCENT WITH EXPONENTIAL LOSS 6.png)

再得到分量模型所占的权重 $\alpha$。

![W13 GRADIENT DESCENT WITH EXPONENTIAL LOSS 7](D:\Files\Learning Materials\Y4\INT303-BigDataAnalytics\MD Pictures\W13 GRADIENT DESCENT WITH EXPONENTIAL LOSS 7.png)



总的过程如下：

![W13 GRADIENT DESCENT WITH EXPONENTIAL LOSS 8](D:\Files\Learning Materials\Y4\INT303-BigDataAnalytics\MD Pictures\W13 GRADIENT DESCENT WITH EXPONENTIAL LOSS 8.png)

训练误差呈指数级快速下降。

### CHOOSING THE LEARNING RATE

与回归的梯度提升不同，我们可以通过优化来解析解决 adaBoost 的最佳学习率：

![W13 lr](D:\Files\Learning Materials\Y4\INT303-BigDataAnalytics\MD Pictures\W13 lr.png)

注：$T$ 是 residuals。