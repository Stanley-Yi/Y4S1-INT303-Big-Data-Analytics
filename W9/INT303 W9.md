# INT303 W9

## NUMERICAL FEATURES

对数字型 feature 的处理。

### Binarization

![W9 BINARIZATION](D:\Files\Learning Materials\Y4\INT303-BigDataAnalytics\MD Pictures\W9 BINARIZATION.png)

注：上面讲观看人数二值化 (binarization) 成了 0 (没人看) 和 1 (有人看)。

### Binning

Binning is to group data according to specific rules

* Achieve discretization of data (实现数据的离散化)
* enhance data stability (增强数据稳定性)
* reduce the risk of overfitting

Binning is very necessary in logistic regression

Tree models do not need to be binned

这里把连续的数据变成不连续的数据，可以增加鲁棒性。



![W9 Binning](D:\Files\Learning Materials\Y4\INT303-BigDataAnalytics\MD Pictures\W9 Binning.png)



From the theoretical stand-point, there are several possible methods of disretization (binning) a continuous variable as follows

* Equal width discretization (等宽离散化)
* Equal Frequency discretization (等频离散化)
* Discretization using decision trees

#### Equal width discretization

The bins or interval limits are determined so that each interval is of the same width.

* Dividing that range into the amount of bins desired (假如年龄是0-100，等距分就是：假如每20年用一个 bin 表示，就有 5 个 bin，0-20，...，80-100)
* Note: if the distribution is skewed, this technique does not improve the spread of the values (这个技术不会改变数据分布，即：如果 0-20 岁的人特别多，画出来的柱状图上，代表 0-20 的 bin 就特别高)

#### Equal Frequency discretization

The boundaries of the intervals are determined so that each bin contains the same number of observations (假如有 100 个人年龄在 0-100，等频分就是：假如分成 5 组，那每组都要 20 个人。例如第一组的20个人，可能在 0-37 岁)

This is a better solution

* We want to spread the values evenly across all bins.

The usual approach is:

* The percentiles
* Quartiles to determine the intervals

#### Discretization using decision trees

Sorting the observations into the tree end leaves, after
training a decision tree.

* Different leaves will contain different number of
  observations
* It does not preserve frequency like equal frequency
  discretization (它不会像等频离散化那样保留频率)

For some datasets, discretization with decision trees can
improve model performance

* Creating monotonic relationships (创建单调关系)
  (already capture some of the predictive power of the
  variable)

Discretization using decision trees 包括使用决策树来确定条柱 (bin) 或连续间隔 (interval) 的最佳拆分点，决策树是一个监督学习。

#### python implementation

* Equal width/frequency discretization: pd.cut()/pd.qcut()

  ![W9 python implementation 1](D:\Files\Learning Materials\Y4\INT303-BigDataAnalytics\MD Pictures\W9 python implementation 1.png)

* Discretization with decision trees

  ![W9 python implementation 2](D:\Files\Learning Materials\Y4\INT303-BigDataAnalytics\MD Pictures\W9 python implementation 2.png)

  注：上面的 Y 相等于 label

  ![W9 python implementation 3](D:\Files\Learning Materials\Y4\INT303-BigDataAnalytics\MD Pictures\W9 python implementation 3.png)

  注：上图只是对 df 和 Y 的统计，其中 0 代表 good，1 代表 bad

### Log Transformation

Compresses the range of large numbers and expand the range of small numbers (压缩大数字的范围并扩展小数字的范围). Eg. The larger x is, the slower log(x) increments.

![W9 LOG TRANSFORMATION](D:\Files\Learning Materials\Y4\INT303-BigDataAnalytics\MD Pictures\W9 LOG TRANSFORMATION.png)

### Scaling

Models that are smooth functions of input features are
sensitive to the scale of the input (eg. Linear Regression) (模型对输入的比例敏感) .
Scale numerical variables into a certain range, dividing values by a normalization constant (no changes in single-feature distribution) (将数值变量缩放到某个范围，将值除以规范化常量，即正则化)，让不同的 feature 拥有相同的 scaling。

Popular techniques:

* Min-Max Scaling
* Standard (Z) Scaling

#### Min-Max Scaling

Squeezes (or stretches) all values within the range of [0, 1] to add robustness to very small standard deviations and preserving zeros for sparse data.

![W9 MIN-MAX SCALING](D:\Files\Learning Materials\Y4\INT303-BigDataAnalytics\MD Pictures\W9 MIN-MAX SCALING.png)

注：sparse data，稀疏数据，指具有大部分未使用元素（不携带任何信息的元素，即 0）的数据。

#### Standard (Z) Scaling

After Standardization, a feature has mean of 0 and variance of 1 (assumption of many learning algorithms)

![W9 STANDARD (Z) SCALING](D:\Files\Learning Materials\Y4\INT303-BigDataAnalytics\MD Pictures\W9 STANDARD (Z) SCALING.png)

注：var(x) 是 x 的方差

### Interaction Features

Simple linear models use a linear combination of the individual input features, x1 , x2 , ... xn to predict the outcome y.
$$
y = w_{1}x_{1} + w_{2}x_{2} + ... +w_{n}x_{n}
$$

An easy way to increase the complexity of the linear model is to create feature combinations (nonlinear features).

Example: Degree 2 interaction features for vector x = (x1, x2) (degree 2 代表最高 2 次方)
$$
y = w_{1}x_{1} + w_{2}x_{2} + \pmb{w_{3}x_{1}x_{2}} + \pmb{w_{4}x_{1}^2} + \pmb{w_{4}x_{2}^2}
$$
![W9 INTERACTION FEATURES](D:\Files\Learning Materials\Y4\INT303-BigDataAnalytics\MD Pictures\W9 INTERACTION FEATURES.png)



## CATEGORICAL FEATURES

High cardinality can create very sparse data (cardinalilty，基数，就是集合中元素的个数)

### One-hot Encoding (OHE)

Transform a categorical feature with m possible values into m binary features.

![W9 ONE-HOT ENCODING (OHE)](D:\Files\Learning Materials\Y4\INT303-BigDataAnalytics\MD Pictures\W9 ONE-HOT ENCODING (OHE).png)

Sparse format 对内存友好 (memory-friendly)，example: “platform=tablet” can be sparsely encoded as “2:1”

### Feature Hashing

Hashes categorical values into vectors with fixed-length (把 categorical value 变成长度固定的向量).
Lower sparsity and higher compression compared to OHE (与 OHE 相比，稀疏度更低，压缩率更高).
Deals with new and rare categorical values (eg: new user-agents)
May introduce collisions (可能出现碰撞)

![W9 FEATURE HASHING](D:\Files\Learning Materials\Y4\INT303-BigDataAnalytics\MD Pictures\W9 FEATURE HASHING.png)

## Above Summary



![W9 NUMERICAL FEATURES SUMMARY](D:\Files\Learning Materials\Y4\INT303-BigDataAnalytics\MD Pictures\W9 NUMERICAL FEATURES SUMMARY.png)

## TEMPORAL FEATURES

### Time zone conversion

Factors to consider:

* 一个国家可能横跨很多时区
* Daylight Saving Time (DST，夏令时)
  * Start and end DST dates

![W9 TIME ZONE CONVERSION](D:\Files\Learning Materials\Y4\INT303-BigDataAnalytics\MD Pictures\W9 TIME ZONE CONVERSION.png)

### Time binning & Trendlines

Time binning

* Apply binning on time data to make it categorial and more general

Trendlines

* 按时间进行 encoding，比如：上个月的支出，上周的支出……
* 显示变化的趋势

## SPATIAL FEATURES

### Spatial variables

Spatial variables encode a location in space, like:

* GPS-coordinates (lat. / long.) - sometimes require
  projection to a different coordinate system
* Street Addresses - require geocoding
* ZipCodes, Cities, States, Countries - usually enriched with the centroid coordinate of the polygon (from external GIS data)

Derived features (派生特征)

* Distance between a user location and searched hotels (Expedia competition)
* Impossible travel speed (fraud detection)

## TEXTUAL FEATURE

### Natural language processing

![W9 NATURAL LANGUAGE PROCESSING](D:\Files\Learning Materials\Y4\INT303-BigDataAnalytics\MD Pictures\W9 NATURAL LANGUAGE PROCESSING.png)

### Text Vectorization

Represent each document as a feature vector in the vector space, where each position represents a word (token) and the contained value is its presence in the document (将每个文档表示为矢量空间中的特征向量，其中每个位置表示一个单词，包含的值是其在文档中的存在).

* BoW (Bag of words)
* TF-IDF (Term Frequency - Inverse Document Frequency)
* Embeddings (eg.Word2Vec, Glove)
* Topic models (e.g LDA)

![W9 TEXT VECTORIZATION](D:\Files\Learning Materials\Y4\INT303-BigDataAnalytics\MD Pictures\W9 TEXT VECTORIZATION.png)

## FEATURE SELECTION

Reduces model complexity and training time

* Filtering - Eg. Correlation our Mutual Information between each feature and the response variable (在每个特征和响应变量之间关联共有的信息)
* Wrapper methods - Expensive, trying to optimize the best subset of features (eg. Stepwise Regression)
* Embedded methods - Feature selection as part of model training process (eg. Feature Importances of Decision Trees or Trees Ensembles)

### Random forest tree

Each tree of the random forest can calculate the importance of a feature according to its ability to increase the pureness of the leaves.
It’s a topic related to how Classification And Regression Trees (CART) work.

![W9 RANDOM FOREST TREE](D:\Files\Learning Materials\Y4\INT303-BigDataAnalytics\MD Pictures\W9 RANDOM FOREST TREE.png)

