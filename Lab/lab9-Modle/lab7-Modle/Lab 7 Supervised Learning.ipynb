{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from preamble import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "#  INT303 Big Data Analytics\n",
    "\n",
    "\n",
    "##  Lab 7 Supervised Learning\n",
    "\n",
    "**XJTLU**<br>\n",
    "**S1 2021**<br>\n",
    "**Instructors:** Jia WANG <br>\n",
    "**Lab Instructor:** Jia WANG <br>\n",
    "\n",
    "\n",
    "**Outline** \n",
    "* Introduction to Linear Regression\n",
    "* Introduction to KNN\n",
    "* Introduction to Decision Tree\n",
    "\n",
    "\n",
    "**Applications** \n",
    "* Wine Classification\n",
    "* Sales Performance Prediction\n",
    "* vehicle prices Prediction\n",
    "* Titanic survival state classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Classification using Scikit-learn\n",
    "## Learn K-Nearest Neighbor(KNN) Classification and build KNN classifier using Python Scikit-learn package.\n",
    "\n",
    "K Nearest Neighbor(KNN) is a very simple, easy to understand, versatile and one of the topmost machine learning algorithms. KNN used in the variety of applications such as finance, healthcare, political science, handwriting detection, image recognition and video recognition. In Credit ratings, financial institutes will predict the credit rating of customers. In loan disbursement, banking institutes will predict whether the loan is safe or risky. In political science, classifying potential voters in two classes will vote or wonâ€™t vote. KNN algorithm used for both classification and regression problems. KNN algorithm based on feature similarity approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors\n",
    "![title](img/knn.jpg \"ShowMyImage\")\n",
    "KNN is a non-parametric and lazy learning algorithm. Non-parametric means there is no assumption for underlying data distribution. In other words, the model structure determined from the dataset. This will be very helpful in practice where most of the real world datasets do not follow mathematical theoretical assumptions. Lazy algorithm means it does not need any training data points for model generation. All training data used in the testing phase. This makes training faster and testing phase slower and costlier. Costly testing phase means time and memory. In the worst case, KNN needs more time to scan all data points and scanning all data points will require more memory for storing training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the KNN algorithm work?\n",
    "\n",
    "In KNN, K is the number of nearest neighbors. The number of neighbors is the core deciding factor. K is generally an odd number if the number of classes is 2. When K=1, then the algorithm is known as the nearest neighbor algorithm. This is the simplest case. Suppose P1 is the point, for which label needs to predict. First, you find the one closest point to P1 and then the label of the nearest point assigned to P1.\n",
    "\n",
    "![title](img/knn_graph.png \"ShowMyImage\")\n",
    "\n",
    "Suppose P1 is the point, for which label needs to predict. First, you find the k closest point to P1 and then classify points by majority vote of its k neighbors. Each object votes for their class and the class with the most votes is taken as the prediction. For finding closest similar points, you find the distance between points using distance measures such as Euclidean distance, Hamming distance, Manhattan distance and Minkowski distance. KNN has the following basic steps:\n",
    "\n",
    "- 1) Calculate distance\n",
    "- 2) Find closest neighbors\n",
    "- 3) Vote for labels\n",
    "\n",
    "![title](img/knn_steps.png \"ShowMyImage\")\n",
    "\n",
    "\n",
    "## Curse of Dimensionality\n",
    "\n",
    "KNN performs better with a lower number of features than a large number of features. You can say that when the number of features increases than it requires more data. Increase in dimension also leads to the problem of overfitting. To avoid overfitting, the needed data will need to grow exponentially as you increase the number of dimensions. This problem of higher dimension is known as the Curse of Dimensionality.\n",
    "To deal with the problem of the curse of dimensionality, you need to perform principal component analysis before applying any machine learning algorithm, or you can also use feature selection approach. Research has shown that in large dimension Euclidean distance is not useful anymore. Therefore, you can prefer other measures such as cosine similarity, which get decidedly less affected by high dimension.\n",
    "\n",
    "## How do you decide the number of neighbors in KNN?\n",
    "\n",
    "Now, you understand the KNN algorithm working mechanism. At this point, the question arises that How to choose the optimal number of neighbors? And what are its effects on the classifier? The number of neighbors(K) in KNN is a hyperparameter that you need choose at the time of model building. You can think of K as a controlling variable for the prediction model.\n",
    "Research has shown that no optimal number of neighbors suits all kind of data sets. Each dataset has it's own requirements. In the case of a small number of neighbors, the noise will have a higher influence on the result, and a large number of neighbors make it computationally expensive. Research has also shown that a small amount of neighbors are most flexible fit which will have low bias but high variance and a large number of neighbors will have a smoother decision boundary which means lower variance but higher bias.\n",
    "Generally, Data scientists choose as an odd number if the number of classes is even. You can also check by generating the model on different values of k and check their performance. You can also try Elbow method here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier  Building in Scikit-learn\n",
    "\n",
    "### KNN Classifier\n",
    "\n",
    "#### Defining dataset\n",
    "\n",
    "Let's first create your own dataset. Here you need two kinds of attributes or columns in your data: Feature and label. The reason for two type of column is \"supervised nature of KNN algorithm\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning features and label variables\n",
    "# First Feature\n",
    "weather=['Sunny','Sunny','Overcast','Rainy','Rainy','Rainy','Overcast','Sunny','Sunny',\n",
    "'Rainy','Sunny','Overcast','Overcast','Rainy']\n",
    "# Second Feature\n",
    "temp=['Hot','Hot','Hot','Mild','Cool','Cool','Cool','Mild','Cool','Mild','Mild','Mild','Hot','Mild']\n",
    "\n",
    "# Label or target varible\n",
    "play=['No','No','Yes','Yes','Yes','No','Yes','No','Yes','Yes','Yes','Yes','Yes','No']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, you have two features (weather and temperature) and one label(play).\n",
    "\n",
    "#### Encoding data columns\n",
    "\n",
    "Various machine learning algorithms require numerical input data, so you need to represent categorical columns in a numerical column.\n",
    "In order to encode this data, you could map each value to a number. e.g. Overcast:0, Rainy:1, and Sunny:2.\n",
    "This process is known as label encoding, and sklearn conveniently will do this for you using Label Encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 0 1 1 1 0 2 2 1 2 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#creating labelEncoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "# Converting string labels into numbers.\n",
    "weather_encoded=le.fit_transform(weather)\n",
    "\n",
    "print(weather_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you imported preprocessing module and created Label Encoder object. Using this LabelEncoder object, you can fit and transform \"weather\" column into the numeric column.\n",
    "Similarly, you can encode temperature and label into numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting string labels into numbers\n",
    "\n",
    "temp_encoded=le.fit_transform(temp)\n",
    "label=le.fit_transform(play)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining Features\n",
    "\n",
    "Here, you will combine multiple columns or features into a single set of data using \"zip\" function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combinig weather and temp into single listof tuples\n",
    "features=list(zip(weather_encoded,temp_encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Model\n",
    "\n",
    "Let's build KNN classifier model.\n",
    "First, import the KNeighborsClassifier module and create KNN classifier object by passing argument number of neighbors in KNeighborsClassifier() function.\n",
    "Then, fit your model on the train set using fit() and perform prediction on the test set using predict()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Train the model using the training sets\n",
    "model.fit(features,label)\n",
    "\n",
    "#Predict Output\n",
    "predicted= model.predict([[0,2]]) # 0:Overcast, 2:Mild\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, you have given input [0,2], where 0 means Overcast weather and 2 means Mild temperature. Model predicts [1], which means play."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN with Multiple Labels\n",
    "\n",
    "Till now, you have learned How to create KNN classifier for two in python using scikit-learn. Now you will learn about KNN with multiple classes.\n",
    "In the model the building part, you can use the wine dataset, which is a very famous multi-class classification problem. This data is the result of a chemical analysis of wines grown in the same region in Italy using three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines.\n",
    "The dataset comprises 13 features ('alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline') and a target (type of cultivars).\n",
    "This data has three types of cultivar classes: 'class_0', 'class_1', and 'class_2'. Here, you can build a model to classify the type of cultivar. The dataset is available in the scikit-learn library, or you can also download it from the UCI Machine Learning Library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Data\n",
    "\n",
    "Let's first load the required wine dataset from scikit-learn datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import scikit-learn dataset library\n",
    "from sklearn import datasets\n",
    "\n",
    "#Load dataset\n",
    "wine = datasets.load_wine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring Data\n",
    "\n",
    "After you have loaded the dataset, you might want to know a little bit more about it. You can check feature and target names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n"
     ]
    }
   ],
   "source": [
    "# print the names of the features\n",
    "print(wine.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class_0' 'class_1' 'class_2']\n"
     ]
    }
   ],
   "source": [
    "# print the label species(class_0, class_1, class_2)\n",
    "print(wine.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check top 5 records of the feature set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.423e+01 1.710e+00 2.430e+00 1.560e+01 1.270e+02 2.800e+00 3.060e+00\n",
      "  2.800e-01 2.290e+00 5.640e+00 1.040e+00 3.920e+00 1.065e+03]\n",
      " [1.320e+01 1.780e+00 2.140e+00 1.120e+01 1.000e+02 2.650e+00 2.760e+00\n",
      "  2.600e-01 1.280e+00 4.380e+00 1.050e+00 3.400e+00 1.050e+03]\n",
      " [1.316e+01 2.360e+00 2.670e+00 1.860e+01 1.010e+02 2.800e+00 3.240e+00\n",
      "  3.000e-01 2.810e+00 5.680e+00 1.030e+00 3.170e+00 1.185e+03]\n",
      " [1.437e+01 1.950e+00 2.500e+00 1.680e+01 1.130e+02 3.850e+00 3.490e+00\n",
      "  2.400e-01 2.180e+00 7.800e+00 8.600e-01 3.450e+00 1.480e+03]\n",
      " [1.324e+01 2.590e+00 2.870e+00 2.100e+01 1.180e+02 2.800e+00 2.690e+00\n",
      "  3.900e-01 1.820e+00 4.320e+00 1.040e+00 2.930e+00 7.350e+02]]\n"
     ]
    }
   ],
   "source": [
    "# print the wine data (top 5 records)\n",
    "print(wine.data[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check records of the target set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "# print the wine labels (0:Class_0, 1:Class_1, 2:Class_3)\n",
    "print(wine.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore it for a bit more. You can also check the shape of the dataset using shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 13)\n"
     ]
    }
   ],
   "source": [
    "# print data(feature)shape\n",
    "print(wine.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178,)\n"
     ]
    }
   ],
   "source": [
    "# print target(or label)shape\n",
    "print(wine.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting Data\n",
    "\n",
    "To understand model performance, dividing the dataset into a training set and a test set is a good strategy.\n",
    "Let's split dataset by using function train_test_split(). You need to pass 3 parameters features, target, and test_set size. Additionally, you can use random_state to select records randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split function\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "## 70% training and 30% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Model for K=5\n",
    "\n",
    "Let's build KNN classifier model for k=5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import knearest neighbors Classifier model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Create KNN Classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "#Train the model using the training sets\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation for k=5\n",
    "\n",
    "Let's estimate, how accurately the classifier or model can predict the type of cultivars.\n",
    "Accuracy can be computed by comparing actual test set values and predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7592592592592593\n"
     ]
    }
   ],
   "source": [
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, you got a classification rate of more or less 70.00%, considered as good accuracy.\n",
    "For further evaluation, you can also create a model for a different number of neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-generating Model for K=7\n",
    "\n",
    "Let's build KNN classifier model for k=7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import knearest neighbors Classifier model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Create KNN Classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "\n",
    "#Train the model using the training sets\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation for k=7\n",
    "\n",
    "Let's again estimate, how accurately the classifier or model can predict the type of cultivars for k=7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, you got a classification rate of more or less 77.00%, considered as good accuracy.\n",
    "Here, you have increased the number of neighbors in the model and accuracy got increased. But, this is not necessary for each case that an increase in many neighbors increases the accuracy. For a more detailed understanding of it, you can refer section \"How to decide the number of neighbors?\" of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros\n",
    "\n",
    "The training phase of K-nearest neighbor classification is much faster compared to other classification algorithms. There is no need to train a model for generalization, That is why KNN is known as the simple and instance-based learning algorithm. KNN can be useful in case of nonlinear data. It can be used with the regression problem. Output value for the object is computed by the average of k closest neighbors value.\n",
    "\n",
    "## Cons\n",
    "\n",
    "The testing phase of K-nearest neighbor classification is slower and costlier in terms of time and memory. It requires large memory for storing the entire training dataset for prediction. KNN requires scaling of data because KNN uses the Euclidean distance between two data points to find nearest neighbors. Euclidean distance is sensitive to magnitudes. The features with high magnitudes will weight more than features with low magnitudes. KNN also not suitable for large dimensional data.\n",
    "\n",
    "## How to improve KNN?\n",
    "\n",
    "For better results, normalizing data on the same scale is highly recommended. Generally, the normalization range considered between 0 and 1. KNN is not suitable for the large dimensional data. In such cases, dimension needs to reduce to improve the performance. Also, handling missing values will help us in improving results.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Congratulations, you have made it to the end of this tutorial!\n",
    "In this tutorial, you have learned the K-Nearest Neighbor algorithm; it's working, eager and lazy learner, the curse of dimensionality, model building and evaluation on wine dataset using Python Scikit-learn package. Also, discussed its advantages, disadvantages, and performance improvement suggestions.\n",
    "I look forward to hearing any feedback or questions. You can ask questions by leaving a comment, and I will try my best to answer it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ref: https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84c1cc2",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Linear Regression\n",
    "## What Didn't We Cover?\n",
    "\n",
    "- Detecting collinearity\n",
    "- Diagnosing model fit\n",
    "- Transforming predictors to fit non-linear relationships\n",
    "- Interaction terms\n",
    "- Assumptions of linear regression\n",
    "- And so much more!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3dbbba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>Radio</th>\n",
       "      <th>Newspaper</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>230.1</td>\n",
       "      <td>37.8</td>\n",
       "      <td>69.2</td>\n",
       "      <td>22.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44.5</td>\n",
       "      <td>39.3</td>\n",
       "      <td>45.1</td>\n",
       "      <td>10.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.2</td>\n",
       "      <td>45.9</td>\n",
       "      <td>69.3</td>\n",
       "      <td>9.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>151.5</td>\n",
       "      <td>41.3</td>\n",
       "      <td>58.5</td>\n",
       "      <td>18.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>180.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>58.4</td>\n",
       "      <td>12.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TV  Radio  Newspaper  Sales\n",
       "1  230.1   37.8       69.2   22.1\n",
       "2   44.5   39.3       45.1   10.4\n",
       "3   17.2   45.9       69.3    9.3\n",
       "4  151.5   41.3       58.5   18.5\n",
       "5  180.8   10.8       58.4   12.9"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# read data into a DataFrame\n",
    "data = pd.read_csv('./data/Advertising.csv', index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f160b5bb",
   "metadata": {},
   "source": [
    "What are the **features**?\n",
    "- TV: advertising dollars spent on TV for a single product in a given market (in thousands of dollars)\n",
    "- Radio: advertising dollars spent on Radio\n",
    "- Newspaper: advertising dollars spent on Newspaper\n",
    "\n",
    "What is the **response**?\n",
    "- Sales: sales of a single product in a given market (in thousands of widgets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d51f370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 4)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the shape of the DataFrame\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa8f17d",
   "metadata": {},
   "source": [
    "There are 200 **observations**, and thus 200 markets in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d737eaa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fae0d09b7b8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the relationship between the features and the response using scatterplots\n",
    "fig, axs = plt.subplots(1, 3, sharey=True)\n",
    "data.plot(kind='scatter', x='TV', y='Sales', ax=axs[0], figsize=(16, 8))\n",
    "data.plot(kind='scatter', x='Radio', y='Sales', ax=axs[1])\n",
    "data.plot(kind='scatter', x='Newspaper', y='Sales', ax=axs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e3b3f5",
   "metadata": {},
   "source": [
    "## Questions About the Advertising Data\n",
    "\n",
    "Let's pretend you work for the company that manufactures and markets this widget. The company might ask you the following: On the basis of this data, how should we spend our advertising money in the future?\n",
    "\n",
    "This general question might lead you to more specific questions:\n",
    "1. Is there a relationship between ads and sales?\n",
    "2. How strong is that relationship?\n",
    "3. Which ad types contribute to sales?\n",
    "4. What is the effect of each ad type of sales?\n",
    "5. Given ad spending in a particular market, can sales be predicted?\n",
    "\n",
    "We will explore these questions below!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f8eafb",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "Simple linear regression is an approach for predicting a **quantitative response** using a **single feature** (or \"predictor\" or \"input variable\"). It takes the following form:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1x$\n",
    "\n",
    "What does each term represent?\n",
    "- $y$ is the response\n",
    "- $x$ is the feature\n",
    "- $\\beta_0$ is the intercept\n",
    "- $\\beta_1$ is the coefficient for x\n",
    "\n",
    "Together, $\\beta_0$ and $\\beta_1$ are called the **model coefficients**. To create your model, you must \"learn\" the values of these coefficients. And once we've learned these coefficients, we can use the model to predict Sales!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46750eaa",
   "metadata": {},
   "source": [
    "## Estimating (\"Learning\") Model Coefficients\n",
    "\n",
    "Generally speaking, coefficients are estimated using the **least squares criterion**, which means we are find the line (mathematically) which minimizes the **sum of squared residuals** (or \"sum of squared errors\"):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe6452c",
   "metadata": {},
   "source": [
    "<img src=\"./img/08_estimating_coefficients.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60040c4",
   "metadata": {},
   "source": [
    "What elements are present in the diagram?\n",
    "- The black dots are the **observed values** of x and y.\n",
    "- The blue line is our **least squares line**.\n",
    "- The red lines are the **residuals**, which are the distances between the observed values and the least squares line.\n",
    "\n",
    "How do the model coefficients relate to the least squares line?\n",
    "- $\\beta_0$ is the **intercept** (the value of $y$ when $x$=0)\n",
    "- $\\beta_1$ is the **slope** (the change in $y$ divided by change in $x$)\n",
    "\n",
    "Here is a graphical depiction of those calculations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1ca423",
   "metadata": {},
   "source": [
    "<img src=\"./img/08_slope_intercept.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebabbb3d",
   "metadata": {},
   "source": [
    "Let's use **Statsmodels** to estimate the model coefficients for the advertising data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc470a2",
   "metadata": {},
   "source": [
    "## How Well Does the Model Fit the data?\n",
    "\n",
    "The most common way to evaluate the overall fit of a linear model is by the **R-squared** value. R-squared is the **proportion of variance explained**, meaning the proportion of variance in the observed data that is explained by the model, or the reduction in error over the **null model**. (The null model just predicts the mean of the observed response, and thus it has an intercept and no slope.)\n",
    "\n",
    "R-squared is between 0 and 1, and higher is better because it means that more variance is explained by the model. Here's an example of what R-squared \"looks like\":"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a30411a",
   "metadata": {},
   "source": [
    "<img src=\"./img/08_r_squared.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc09630",
   "metadata": {},
   "source": [
    "You can see that the **blue line** explains some of the variance in the data (R-squared=0.54), the **green line** explains more of the variance (R-squared=0.64), and the **red line** fits the training data even further (R-squared=0.66). (Does the red line look like it's overfitting?)\n",
    "\n",
    "Let's calculate the R-squared value for our simple linear model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2109c415",
   "metadata": {},
   "source": [
    "Is that a \"good\" R-squared value? It's hard to say. The threshold for a good R-squared value depends widely on the domain. Therefore, it's most useful as a tool for **comparing different models**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30d24db",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "Simple linear regression can easily be extended to include multiple features. This is called **multiple linear regression**:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n$\n",
    "\n",
    "Each $x$ represents a different feature, and each feature has its own coefficient. In this case:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1 \\times TV + \\beta_2 \\times Radio + \\beta_3 \\times Newspaper$\n",
    "\n",
    "Let's use Statsmodels to estimate these coefficients:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c45d4f",
   "metadata": {},
   "source": [
    "## Linear Regression in scikit-learn\n",
    "\n",
    "Let's redo some of the Statsmodels code above in scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "47f7eae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.938889369459412\n",
      "[ 0.04576465  0.18853002 -0.00103749]\n"
     ]
    }
   ],
   "source": [
    "# create X and y\n",
    "feature_cols = ['TV', 'Radio', 'Newspaper']\n",
    "X = data[feature_cols]\n",
    "y = data.Sales\n",
    "\n",
    "# follow the usual sklearn pattern: import, instantiate, fit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "lm.fit(X, y)\n",
    "\n",
    "# print intercept and coefficients\n",
    "print (lm.intercept_)\n",
    "print (lm.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "319e0350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x7fae0c8789c8>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pair the feature names with the coefficients\n",
    "zip(feature_cols, lm.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1b2cb06e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.20266701])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# predict for a new observation\n",
    "lm.predict(np.array([100, 25, 25]).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a587b1c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8972106381789521"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the R-squared\n",
    "lm.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d9f971",
   "metadata": {},
   "source": [
    "Note that **p-values** and **confidence intervals** are not (easily) accessible through scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4525462",
   "metadata": {},
   "source": [
    "## Handling Categorical Predictors with Two Categories\n",
    "\n",
    "Up to now, all of our predictors have been numeric. What if one of our predictors was categorical?\n",
    "\n",
    "Let's create a new feature called **Size**, and randomly assign observations to be **small or large**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7f15b6d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>Radio</th>\n",
       "      <th>Newspaper</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>230.1</td>\n",
       "      <td>37.8</td>\n",
       "      <td>69.2</td>\n",
       "      <td>22.1</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44.5</td>\n",
       "      <td>39.3</td>\n",
       "      <td>45.1</td>\n",
       "      <td>10.4</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.2</td>\n",
       "      <td>45.9</td>\n",
       "      <td>69.3</td>\n",
       "      <td>9.3</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>151.5</td>\n",
       "      <td>41.3</td>\n",
       "      <td>58.5</td>\n",
       "      <td>18.5</td>\n",
       "      <td>small</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>180.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>58.4</td>\n",
       "      <td>12.9</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TV  Radio  Newspaper  Sales   Size\n",
       "1  230.1   37.8       69.2   22.1  large\n",
       "2   44.5   39.3       45.1   10.4  small\n",
       "3   17.2   45.9       69.3    9.3  small\n",
       "4  151.5   41.3       58.5   18.5  small\n",
       "5  180.8   10.8       58.4   12.9  large"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# set a seed for reproducibility\n",
    "np.random.seed(12345)\n",
    "\n",
    "# create a Series of booleans in which roughly half are True\n",
    "nums = np.random.rand(len(data))\n",
    "mask_large = nums > 0.5\n",
    "\n",
    "# initially set Size to small, then change roughly half to be large\n",
    "data['Size'] = 'small'\n",
    "data.loc[mask_large, 'Size'] = 'large'\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b37b9fc",
   "metadata": {},
   "source": [
    "For scikit-learn, we need to represent all data **numerically**. If the feature only has two categories, we can simply create a **dummy variable** that represents the categories as a binary value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "becb7454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>Radio</th>\n",
       "      <th>Newspaper</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Size</th>\n",
       "      <th>IsLarge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>230.1</td>\n",
       "      <td>37.8</td>\n",
       "      <td>69.2</td>\n",
       "      <td>22.1</td>\n",
       "      <td>large</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44.5</td>\n",
       "      <td>39.3</td>\n",
       "      <td>45.1</td>\n",
       "      <td>10.4</td>\n",
       "      <td>small</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.2</td>\n",
       "      <td>45.9</td>\n",
       "      <td>69.3</td>\n",
       "      <td>9.3</td>\n",
       "      <td>small</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>151.5</td>\n",
       "      <td>41.3</td>\n",
       "      <td>58.5</td>\n",
       "      <td>18.5</td>\n",
       "      <td>small</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>180.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>58.4</td>\n",
       "      <td>12.9</td>\n",
       "      <td>large</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TV  Radio  Newspaper  Sales   Size  IsLarge\n",
       "1  230.1   37.8       69.2   22.1  large        1\n",
       "2   44.5   39.3       45.1   10.4  small        0\n",
       "3   17.2   45.9       69.3    9.3  small        0\n",
       "4  151.5   41.3       58.5   18.5  small        0\n",
       "5  180.8   10.8       58.4   12.9  large        1"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a new Series called IsLarge\n",
    "data['IsLarge'] = data.Size.map({'small':0, 'large':1})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4285b50b",
   "metadata": {},
   "source": [
    "Let's redo the multiple linear regression and include the **IsLarge** predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2a1287aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x7fae0d0fd308>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create X and y\n",
    "feature_cols = ['TV', 'Radio', 'Newspaper', 'IsLarge']\n",
    "X = data[feature_cols]\n",
    "y = data.Sales\n",
    "\n",
    "# instantiate, fit\n",
    "lm = LinearRegression()\n",
    "lm.fit(X, y)\n",
    "\n",
    "# print coefficients\n",
    "zip(feature_cols, lm.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5bc2d5",
   "metadata": {},
   "source": [
    "How do we interpret the **IsLarge coefficient**? For a given amount of TV/Radio/Newspaper ad spending, being a large market is associated with an average **increase** in Sales of 57.42 widgets (as compared to a Small market, which is called the **baseline level**).\n",
    "\n",
    "What if we had reversed the 0/1 coding and created the feature 'IsSmall' instead? The coefficient would be the same, except it would be **negative instead of positive**. As such, your choice of category for the baseline does not matter, all that changes is your **interpretation** of the coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b9ff4e",
   "metadata": {},
   "source": [
    "## Handling Categorical Predictors with More than Two Categories\n",
    "\n",
    "Let's create a new feature called **Area**, and randomly assign observations to be **rural, suburban, or urban**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "af879d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>Radio</th>\n",
       "      <th>Newspaper</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Size</th>\n",
       "      <th>IsLarge</th>\n",
       "      <th>Area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>230.1</td>\n",
       "      <td>37.8</td>\n",
       "      <td>69.2</td>\n",
       "      <td>22.1</td>\n",
       "      <td>large</td>\n",
       "      <td>1</td>\n",
       "      <td>rural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44.5</td>\n",
       "      <td>39.3</td>\n",
       "      <td>45.1</td>\n",
       "      <td>10.4</td>\n",
       "      <td>small</td>\n",
       "      <td>0</td>\n",
       "      <td>urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.2</td>\n",
       "      <td>45.9</td>\n",
       "      <td>69.3</td>\n",
       "      <td>9.3</td>\n",
       "      <td>small</td>\n",
       "      <td>0</td>\n",
       "      <td>rural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>151.5</td>\n",
       "      <td>41.3</td>\n",
       "      <td>58.5</td>\n",
       "      <td>18.5</td>\n",
       "      <td>small</td>\n",
       "      <td>0</td>\n",
       "      <td>urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>180.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>58.4</td>\n",
       "      <td>12.9</td>\n",
       "      <td>large</td>\n",
       "      <td>1</td>\n",
       "      <td>suburban</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TV  Radio  Newspaper  Sales   Size  IsLarge      Area\n",
       "1  230.1   37.8       69.2   22.1  large        1     rural\n",
       "2   44.5   39.3       45.1   10.4  small        0     urban\n",
       "3   17.2   45.9       69.3    9.3  small        0     rural\n",
       "4  151.5   41.3       58.5   18.5  small        0     urban\n",
       "5  180.8   10.8       58.4   12.9  large        1  suburban"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set a seed for reproducibility\n",
    "np.random.seed(123456)\n",
    "\n",
    "# assign roughly one third of observations to each group\n",
    "nums = np.random.rand(len(data))\n",
    "mask_suburban = (nums > 0.33) & (nums < 0.66)\n",
    "mask_urban = nums > 0.66\n",
    "data['Area'] = 'rural'\n",
    "data.loc[mask_suburban, 'Area'] = 'suburban'\n",
    "data.loc[mask_urban, 'Area'] = 'urban'\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9235c71b",
   "metadata": {},
   "source": [
    "We have to represent Area numerically, but we can't simply code it as 0=rural, 1=suburban, 2=urban because that would imply an **ordered relationship** between suburban and urban (and thus urban is somehow \"twice\" the suburban category).\n",
    "\n",
    "Instead, we create **another dummy variable**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "425ed0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>Radio</th>\n",
       "      <th>Newspaper</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Size</th>\n",
       "      <th>IsLarge</th>\n",
       "      <th>Area</th>\n",
       "      <th>Area_suburban</th>\n",
       "      <th>Area_urban</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>230.1</td>\n",
       "      <td>37.8</td>\n",
       "      <td>69.2</td>\n",
       "      <td>22.1</td>\n",
       "      <td>large</td>\n",
       "      <td>1</td>\n",
       "      <td>rural</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44.5</td>\n",
       "      <td>39.3</td>\n",
       "      <td>45.1</td>\n",
       "      <td>10.4</td>\n",
       "      <td>small</td>\n",
       "      <td>0</td>\n",
       "      <td>urban</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.2</td>\n",
       "      <td>45.9</td>\n",
       "      <td>69.3</td>\n",
       "      <td>9.3</td>\n",
       "      <td>small</td>\n",
       "      <td>0</td>\n",
       "      <td>rural</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>151.5</td>\n",
       "      <td>41.3</td>\n",
       "      <td>58.5</td>\n",
       "      <td>18.5</td>\n",
       "      <td>small</td>\n",
       "      <td>0</td>\n",
       "      <td>urban</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>180.8</td>\n",
       "      <td>10.8</td>\n",
       "      <td>58.4</td>\n",
       "      <td>12.9</td>\n",
       "      <td>large</td>\n",
       "      <td>1</td>\n",
       "      <td>suburban</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      TV  Radio  Newspaper  Sales   Size  IsLarge      Area  Area_suburban  \\\n",
       "1  230.1   37.8       69.2   22.1  large        1     rural              0   \n",
       "2   44.5   39.3       45.1   10.4  small        0     urban              0   \n",
       "3   17.2   45.9       69.3    9.3  small        0     rural              0   \n",
       "4  151.5   41.3       58.5   18.5  small        0     urban              0   \n",
       "5  180.8   10.8       58.4   12.9  large        1  suburban              1   \n",
       "\n",
       "   Area_urban  \n",
       "1           0  \n",
       "2           1  \n",
       "3           0  \n",
       "4           1  \n",
       "5           0  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create three dummy variables using get_dummies, then exclude the first dummy column\n",
    "area_dummies = pd.get_dummies(data.Area, prefix='Area').iloc[:, 1:]\n",
    "\n",
    "# concatenate the dummy variable columns onto the original DataFrame (axis=0 means rows, axis=1 means columns)\n",
    "data = pd.concat([data, area_dummies], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1536717",
   "metadata": {},
   "source": [
    "Here is how we interpret the coding:\n",
    "- **rural** is coded as Area_suburban=0 and Area_urban=0\n",
    "- **suburban** is coded as Area_suburban=1 and Area_urban=0\n",
    "- **urban** is coded as Area_suburban=0 and Area_urban=1\n",
    "\n",
    "Why do we only need **two dummy variables, not three?** Because two dummies captures all of the information about the Area feature, and implicitly defines rural as the baseline level. (In general, if you have a categorical feature with k levels, you create k-1 dummy variables.)\n",
    "\n",
    "If this is confusing, think about why we only needed one dummy variable for Size (IsLarge), not two dummy variables (IsSmall and IsLarge).\n",
    "\n",
    "Let's include the two new dummy variables in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8c0404bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('TV', 0.04574401036331374)\n",
      "('Radio', 0.18786669552525798)\n",
      "('Newspaper', -0.0010876977267109122)\n",
      "('IsLarge', 0.07739660749747915)\n",
      "('Area_suburban', -0.10656299015958612)\n",
      "('Area_urban', 0.26813802165220074)\n"
     ]
    }
   ],
   "source": [
    "# create X and y\n",
    "feature_cols = ['TV', 'Radio', 'Newspaper', 'IsLarge', 'Area_suburban', 'Area_urban']\n",
    "X = data[feature_cols]\n",
    "y = data.Sales\n",
    "\n",
    "# instantiate, fit\n",
    "lm = LinearRegression()\n",
    "lm.fit(X, y)\n",
    "\n",
    "# print coefficients\n",
    "for i in zip(feature_cols, lm.coef_):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79caded",
   "metadata": {},
   "source": [
    "How do we interpret the coefficients?\n",
    "- Holding all other variables fixed, being a **suburban** area is associated with an average **decrease** in Sales of 106.56 widgets (as compared to the baseline level, which is rural).\n",
    "- Being an **urban** area is associated with an average **increase** in Sales of 268.13 widgets (as compared to rural).\n",
    "\n",
    "**A final note about dummy encoding:** If you have categories that can be ranked (i.e., strongly disagree, disagree, neutral, agree, strongly agree), you can potentially use a single dummy variable and represent the categories numerically (such as 1, 2, 3, 4, 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1784839f",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- To go much more in-depth on linear regression, read Chapter 3 of [An Introduction to Statistical Learning](https://www.statlearning.com/), from which this lesson was adapted. Alternatively, watch the [related videos](http://www.dataschool.io/15-hours-of-expert-machine-learning-videos/) or read my [quick reference guide](http://www.dataschool.io/applying-and-interpreting-linear-regression/) to the key points in that chapter.\n",
    "- To learn more about Statsmodels and how to interpret the output, DataRobot has some decent posts on [simple linear regression](http://www.datarobot.com/blog/ordinary-least-squares-in-python/) and [multiple linear regression](http://www.datarobot.com/blog/multiple-regression-using-statsmodels/).\n",
    "- This [introduction to linear regression](http://people.duke.edu/~rnau/regintro.htm) is much more detailed and mathematically thorough, and includes lots of good advice.\n",
    "- This is a relatively quick post on the [assumptions of linear regression](http://pareonline.net/getvn.asp?n=2&v=8)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989260bc",
   "metadata": {},
   "source": [
    "# Introduction to Decision Trees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cb81d5",
   "metadata": {},
   "source": [
    "## Regression trees\n",
    "\n",
    "Let's look at a simple example to motivate our learning.\n",
    "\n",
    "Our goal is to **predict a baseball player's Salary** based on **Years** (number of years playing in the major leagues) and **Hits** (number of hits he made in the previous year). Here is the training data, represented visually (low salary is blue/green, high salary is red/yellow):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61f372a",
   "metadata": {},
   "source": [
    "<img src=\"./img/15_salary_color.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d852c498",
   "metadata": {},
   "source": [
    "**How might you \"stratify\" or \"segment\" the feature space into regions, based on salary?** Intuitively, you want to **maximize** the similarity (or \"homogeneity\") within a given region, and **minimize** the similarity between different regions.\n",
    "\n",
    "Below is a regression tree that has been fit to the data by a computer. (We will talk later about how the fitting algorithm actually works.) Note that  Salary is measured in thousands and has been log-transformed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b85d11f",
   "metadata": {},
   "source": [
    "<img src=\"./img/15_salary_tree.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d722ac82",
   "metadata": {},
   "source": [
    "**How do we make Salary predictions (for out-of-sample data) using a decision tree?**\n",
    "\n",
    "- Start at the top, and examine the first \"splitting rule\" (Years < 4.5).\n",
    "- If the rule is True for a given player, follow the left branch. If the rule is False, follow the right branch.\n",
    "- Continue until reaching the bottom. The predicted Salary is the number in that particular \"bucket\".\n",
    "- *Side note:* Years and Hits are both integers, but the convention is to label these rules using the midpoint between adjacent values.\n",
    "\n",
    "Examples predictions:\n",
    "\n",
    "- Years=3, then predict 5.11 ($\\$1000 \\times e^{5.11} \\approx \\$166000$)\n",
    "- Years=5 and Hits=100, then predict 6.00 ($\\$1000 \\times e^{6.00} \\approx \\$403000$)\n",
    "- Years=8 and Hits=120, then predict 6.74 ($\\$1000 \\times e^{6.74} \\approx \\$846000$)\n",
    "\n",
    "**How did we come up with the numbers at the bottom of the tree?** Each number is just the **mean Salary in the training data** of players who fit that criteria. Here's the same diagram as before, split into the three regions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10ad111",
   "metadata": {},
   "source": [
    "<img src=\"./img/15_salary_regions.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc9b38a",
   "metadata": {},
   "source": [
    "This diagram is essentially a combination of the two previous diagrams (except that the observations are no longer color-coded). In $R_1$, the mean log Salary was 5.11. In $R_2$, the mean log Salary was 6.00. In $R_3$, the mean log Salary was 6.74. Thus, those values are used to predict out-of-sample data.\n",
    "\n",
    "Let's introduce some terminology:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543f8575",
   "metadata": {},
   "source": [
    "<img src=\"./img/15_salary_tree_annotated.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06871ae5",
   "metadata": {},
   "source": [
    "**How might you interpret the \"meaning\" of this tree?**\n",
    "\n",
    "- Years is the most important factor determining Salary, with a lower number of Years corresponding to a lower Salary.\n",
    "- For a player with a lower number of Years, Hits is not an important factor determining Salary.\n",
    "- For a player with a higher number of Years, Hits is an important factor determining Salary, with a greater number of Hits corresponding to a higher Salary.\n",
    "\n",
    "What we have seen so far hints at the advantages and disadvantages of decision trees:\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Highly interpretable\n",
    "- Can be displayed graphically\n",
    "- Prediction is fast\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "- Predictive accuracy is not as high as some supervised learning methods\n",
    "- Can easily overfit the training data (high variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0ead59",
   "metadata": {},
   "source": [
    "## Building a regression tree by hand\n",
    "\n",
    "How do you build a decision tree? You're going to find out by building one in pairs!\n",
    "\n",
    "Your training data is a tiny dataset of [used vehicle sale prices](https://raw.githubusercontent.com/justmarkham/DAT4/master/data/used_vehicles.csv). Your goal is to predict Price for out-of-sample data. Here are your instructions:\n",
    "\n",
    "- Read the data into Pandas.\n",
    "- Explore the data by sorting, plotting, or split-apply-combine (aka `group_by`).\n",
    "- Decide which feature is the most important predictor, and use that to make your first split. (Only binary splits are allowed!)\n",
    "- After making your first split, you should actually split your data in Pandas into two parts, and then explore each part to figure out what other splits to make.\n",
    "- Stop making splits once you are convinced that it strikes a good balance between underfitting and overfitting. (As always, your goal is to build a model that generalizes well!)\n",
    "- You are allowed to split on the same variable multiple times!\n",
    "- Draw your tree, making sure to label your leaves with the mean Price for the observations in that \"bucket\".\n",
    "- When you're finished, review your tree to make sure nothing is backwards. (Remember: follow the left branch if the rule is true, and follow the right branch if the rule is false.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4894c62b",
   "metadata": {},
   "source": [
    "## How does a computer build a regression tree?\n",
    "\n",
    "The ideal approach would be for the computer to consider every possible partition of the feature space. However, this is computationally infeasible, so instead an approach is used called **recursive binary splitting:**\n",
    "\n",
    "- Begin at the top of the tree.\n",
    "- For every single predictor, examine every possible cutpoint, and choose the predictor and cutpoint such that the resulting tree has the **lowest possible mean squared error (MSE)**. Make that split.\n",
    "- Repeat the examination for the two resulting regions, and again make a single split (in one of the regions) to minimize the MSE.\n",
    "- Keep repeating this process until a stopping criteria is met.\n",
    "\n",
    "**How does it know when to stop?**\n",
    "\n",
    "1. We could define a stopping criterion, such as a **maximum depth** of the tree or the **minimum number of samples in the leaf**.\n",
    "2. We could grow the tree deep, and then \"prune\" it back using a method such as \"cost complexity pruning\" (aka \"weakest link pruning\").\n",
    "\n",
    "Method 2 involves setting a tuning parameter that penalizes the tree for having too many leaves. As the parameter is increased, branches automatically get pruned from the tree, resulting in smaller and smaller trees. The tuning parameter can be selected through cross-validation.\n",
    "\n",
    "Note: **Method 2 is not currently supported by scikit-learn**, and so we will use Method 1 instead.\n",
    "\n",
    "Here's an example of an **unpruned tree**, and a comparison of the training, test, and cross-validation errors for trees with different numbers of leaves:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3000a555",
   "metadata": {},
   "source": [
    "<img src=\"./img/15_salary_unpruned.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51c5d71",
   "metadata": {},
   "source": [
    "As you can see, the **training error** continues to go down as the tree size increases, but the lowest **cross-validation error** occurs for a tree with 3 leaves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe20b6e",
   "metadata": {},
   "source": [
    "## Building a regression tree in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5ba6803d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>miles</th>\n",
       "      <th>doors</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22000</td>\n",
       "      <td>2012</td>\n",
       "      <td>13000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14000</td>\n",
       "      <td>2010</td>\n",
       "      <td>30000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13000</td>\n",
       "      <td>2010</td>\n",
       "      <td>73500</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9500</td>\n",
       "      <td>2009</td>\n",
       "      <td>78000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9000</td>\n",
       "      <td>2007</td>\n",
       "      <td>47000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4000</td>\n",
       "      <td>2006</td>\n",
       "      <td>124000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3000</td>\n",
       "      <td>2004</td>\n",
       "      <td>177000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2000</td>\n",
       "      <td>2004</td>\n",
       "      <td>209000</td>\n",
       "      <td>4</td>\n",
       "      <td>truck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3000</td>\n",
       "      <td>2003</td>\n",
       "      <td>138000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1900</td>\n",
       "      <td>2003</td>\n",
       "      <td>160000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2500</td>\n",
       "      <td>2003</td>\n",
       "      <td>190000</td>\n",
       "      <td>2</td>\n",
       "      <td>truck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5000</td>\n",
       "      <td>2001</td>\n",
       "      <td>62000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1800</td>\n",
       "      <td>1999</td>\n",
       "      <td>163000</td>\n",
       "      <td>2</td>\n",
       "      <td>truck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1300</td>\n",
       "      <td>1997</td>\n",
       "      <td>138000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    price  year   miles  doors   type\n",
       "0   22000  2012   13000      2    car\n",
       "1   14000  2010   30000      2    car\n",
       "2   13000  2010   73500      4    car\n",
       "3    9500  2009   78000      4    car\n",
       "4    9000  2007   47000      4    car\n",
       "5    4000  2006  124000      2    car\n",
       "6    3000  2004  177000      4    car\n",
       "7    2000  2004  209000      4  truck\n",
       "8    3000  2003  138000      2    car\n",
       "9    1900  2003  160000      4    car\n",
       "10   2500  2003  190000      2  truck\n",
       "11   5000  2001   62000      4    car\n",
       "12   1800  1999  163000      2  truck\n",
       "13   1300  1997  138000      4    car"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# read in vehicle data\n",
    "vehicles = pd.read_csv('./data/used_vehicles.csv')\n",
    "\n",
    "# print out data\n",
    "vehicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4fb24b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert car to 0 and truck to 1\n",
    "vehicles['type'] = vehicles.type.map({'car':0, 'truck':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1e77f4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select feature columns (every column except for the 0th column)\n",
    "feature_cols = vehicles.columns[1:]\n",
    "\n",
    "# define X (features) and y (response)\n",
    "X = vehicles[feature_cols]\n",
    "y = vehicles.price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c45e6eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train/test\n",
    "#from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d90d6a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    year   miles  doors  type\n",
      "10  2003  190000      2     1\n",
      "4   2007   47000      4     0\n",
      "1   2010   30000      2     0\n",
      "12  1999  163000      2     1\n",
      "0   2012   13000      2     0\n",
      "13  1997  138000      4     0\n",
      "9   2003  160000      4     0\n",
      "8   2003  138000      2     0\n",
      "11  2001   62000      4     0\n",
      "5   2006  124000      2     0\n",
      "10     2500\n",
      "4      9000\n",
      "1     14000\n",
      "12     1800\n",
      "0     22000\n",
      "13     1300\n",
      "9      1900\n",
      "8      3000\n",
      "11     5000\n",
      "5      4000\n",
      "Name: price, dtype: int64\n",
      "   year   miles  doors  type\n",
      "3  2009   78000      4     0\n",
      "7  2004  209000      4     1\n",
      "6  2004  177000      4     0\n",
      "2  2010   73500      4     0\n",
      "3     9500\n",
      "7     2000\n",
      "6     3000\n",
      "2    13000\n",
      "Name: price, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print out each of the arrays\n",
    "print (X_train)\n",
    "print (y_train)\n",
    "print (X_test)\n",
    "print (y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a3befaff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      presort=False, random_state=1, splitter='best')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import class, instantiate estimator, fit with training set\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "treereg = DecisionTreeRegressor(random_state=1)\n",
    "treereg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2a89292c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000. 1900. 1900. 5000.]\n",
      "3     9500\n",
      "7     2000\n",
      "6     3000\n",
      "2    13000\n",
      "Name: price, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# make predictions\n",
    "preds = treereg.predict(X_test)\n",
    "\n",
    "# print predictions and actual values\n",
    "print (preds)\n",
    "print (y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "353a6be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4622.4993239588475"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print RMSE\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "np.sqrt(metrics.mean_squared_error(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8f076521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use cross-validation to find best max_depth\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "91a96c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in sqrt\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try max_depth=2\n",
    "treereg = DecisionTreeRegressor(max_depth=2, random_state=1)\n",
    "scores = cross_val_score(treereg, X, y, cv=3, scoring='neg_mean_squared_error')\n",
    "np.mean(np.sqrt(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c32baccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4592.155425575525"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try max_depth=3\n",
    "treereg = DecisionTreeRegressor(max_depth=3, random_state=1)\n",
    "scores = cross_val_score(treereg, X, y, cv=3, scoring='neg_mean_squared_error')\n",
    "np.mean(np.sqrt(-scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1bb11335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4704.005269479739"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try max_depth=4\n",
    "treereg = DecisionTreeRegressor(max_depth=4, random_state=1)\n",
    "scores = cross_val_score(treereg, X, y, cv=3, scoring='neg_mean_squared_error')\n",
    "np.mean(np.sqrt(-scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ca34397d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=3, max_features=None,\n",
       "                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "                      min_impurity_split=None, min_samples_leaf=1,\n",
       "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                      presort=False, random_state=1, splitter='best')"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_depth=3 was best, so fit a tree using that parameter with ALL DATA\n",
    "treereg = DecisionTreeRegressor(max_depth=3, random_state=1)\n",
    "treereg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "16081983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>year</td>\n",
       "      <td>0.798744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>miles</td>\n",
       "      <td>0.201256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doors</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>type</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  feature  importance\n",
       "0    year    0.798744\n",
       "1   miles    0.201256\n",
       "2   doors    0.000000\n",
       "3    type    0.000000"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the \"Gini importance\" of each feature: the (normalized) total reduction of MSE brought by that feature\n",
    "pd.DataFrame({'feature':feature_cols, 'importance':treereg.feature_importances_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1a4cbc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Graphviz file\n",
    "#from sklearn.tree import export_graphviz\n",
    "#with open(\"./img/15_vehicles.dot\", 'wb') as f:\n",
    "    #f = export_graphviz(treereg, out_file=f, feature_names=feature_cols)\n",
    "# at the command line, run this to convert to PNG:\n",
    "#dot -Tpng 15_vehicles.dot -o 15_vehicles.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec3f645",
   "metadata": {},
   "source": [
    "<img src=\"./img/15_vehicles.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cb47f1",
   "metadata": {},
   "source": [
    "## Interpreting a tree diagram\n",
    "\n",
    "How do we read this decision tree?\n",
    "\n",
    "**Internal nodes:**\n",
    "\n",
    "- \"samples\" is the number of observations in that node before splitting\n",
    "- \"mse\" is the mean squared error calculated by comparing the actual response values in that node against the mean response value in that node\n",
    "- first line is the condition used to split that node (go left if true, go right if false)\n",
    "\n",
    "**Leaves:**\n",
    "\n",
    "- \"samples\" is the number of observations in that node\n",
    "- \"value\" is the mean response value in that node\n",
    "- \"mse\" is the mean squared error calculated by comparing the actual response values in that node against \"value\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebeeba9",
   "metadata": {},
   "source": [
    "## Predicting for out-of-sample data\n",
    "\n",
    "How accurate is scikit-learn's regression tree at predicting the out-of-sample data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f5b961c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>miles</th>\n",
       "      <th>doors</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3000</td>\n",
       "      <td>2003</td>\n",
       "      <td>130000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6000</td>\n",
       "      <td>2005</td>\n",
       "      <td>82500</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12000</td>\n",
       "      <td>2010</td>\n",
       "      <td>60000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price  year   miles  doors  type\n",
       "0   3000  2003  130000      4     1\n",
       "1   6000  2005   82500      4     0\n",
       "2  12000  2010   60000      2     0"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in out-of-sample data\n",
    "oos = pd.read_csv('./data/used_vehicles_oos.csv')\n",
    "\n",
    "# convert car to 0 and truck to 1\n",
    "oos['type'] = oos.type.map({'car':0, 'truck':1})\n",
    "\n",
    "# print data\n",
    "oos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6865233b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define X and y\n",
    "X_oos = oos[feature_cols]\n",
    "y_oos = oos.price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bd542b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4000.  5000. 13500.]\n",
      "[ 3000  6000 12000]\n"
     ]
    }
   ],
   "source": [
    "# make predictions on out-of-sample data\n",
    "preds = treereg.predict(X_oos)\n",
    "\n",
    "# print predictions and actual values\n",
    "print (preds)\n",
    "print (y_oos.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1c00d894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1190.2380714238084"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print RMSE\n",
    "np.sqrt(metrics.mean_squared_error(y_oos, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b5fd5c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1190.2380714238084"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print RMSE for the tree you created!\n",
    "your_preds = [4000, 5000, 13500]\n",
    "np.sqrt(metrics.mean_squared_error(y_oos, your_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870a2a3e",
   "metadata": {},
   "source": [
    "## Classification trees\n",
    "\n",
    "Classification trees are very similar to regression trees. Here is a quick comparison:\n",
    "\n",
    "|regression trees|classification trees|\n",
    "|---|---|\n",
    "|predict a continuous response|predict a categorical response|\n",
    "|predict using mean response of each leaf|predict using most commonly occuring class of each leaf|\n",
    "|splits are chosen to minimize MSE|splits are chosen to minimize a different criterion (discussed below)|\n",
    "\n",
    "Note that classification trees easily handle **more than two response classes**! (How have other classification models we've seen handled this scenario?)\n",
    "\n",
    "Here's an **example of a classification tree**, which predicts whether or not a patient who presented with chest pain has heart disease:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1099db7",
   "metadata": {},
   "source": [
    "<img src=\"./img/15_heart_tree.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce344aa",
   "metadata": {},
   "source": [
    "## Splitting criteria for classification trees\n",
    "\n",
    "Here are common options for the splitting criteria:\n",
    "\n",
    "- **classification error rate:** fraction of training observations in a region that don't belong to the most common class\n",
    "- **Gini index:** measure of total variance across classes in a region\n",
    "- **cross-entropy:** numerically similar to Gini index, but uses logarithms\n",
    "\n",
    "Which to use?\n",
    "\n",
    "- When growing a tree, Gini index and cross-entropy are better measures of \"node purity\" than classification error rate. The Gini index is faster to compute than cross-entropy, so it is generally preferred (and is used by scikit-learn by default).\n",
    "- When pruning a tree, classification error rate is preferable in order to maximize predictive accuracy.\n",
    "\n",
    "Why do some splits result in leaves with the same predicted class?\n",
    "\n",
    "- The split was performed to increase node purity, even though it didn't reduce the classification error.\n",
    "- Node purity is important because we're interested in the class proportions among the observations in each region."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1a94cf",
   "metadata": {},
   "source": [
    "## Handling categorical predictors\n",
    "\n",
    "Some implementations of classification trees will allow you to handle categorical predictors **without creating dummy variables**. When splitting on a categorical predictor, they will try splitting on **every possible combination of categories** to find the best split. In the example above, \"ChestPain:bc\" means that the left-hand branch consists of observations with the second and third ChestPain categories, and the right-hand branch consists of remaining observations.\n",
    "\n",
    "**Unfortunately, scikit-learn's classification tree implementation does not support this approach.** Instead, here's how you can handle categorical predictors:\n",
    "\n",
    "- If a predictor only has **two possible values**, code it as a single binary variable (0 or 1). Since it's treated as a number, splits will naturally occur at 0.5.\n",
    "- If a predictor has **three or more possible values that are ordered**, code it as a single variable (1, 2, 3, etc). Splits will naturally occur at 1.5, 2.5, etc.\n",
    "- If a predictor has **three or more possible values that are unordered**, create dummy variables and drop one level as usual. The decision tree won't know that the dummy variables are related to one another, but that shouldn't matter in terms of predictive accuracy.\n",
    "- If a predictor has **thousands of possible unordered values**, then it may be best to code it as a single variable (1, 2, 3, etc) instead of using dummy variables to minimize the size of the resulting model. ([reference](http://stackoverflow.com/a/18736132/1636598))\n",
    "\n",
    "We'll see examples of these strategies below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7bfe88",
   "metadata": {},
   "source": [
    "## Building a classification tree in scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815ed326",
   "metadata": {},
   "source": [
    "We'll build a classification tree using the [Titanic data](https://www.kaggle.com/c/titanic-gettingStarted/data) provided by Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8a94d1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>male</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master. Gosta Leonard</td>\n",
       "      <td>male</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>female</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>female</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>237736</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass                                               name  \\\n",
       "0         0       3                            Braund, Mr. Owen Harris   \n",
       "1         1       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n",
       "2         1       3                             Heikkinen, Miss. Laina   \n",
       "3         1       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n",
       "4         0       3                           Allen, Mr. William Henry   \n",
       "5         0       3                                   Moran, Mr. James   \n",
       "6         0       1                            McCarthy, Mr. Timothy J   \n",
       "7         0       3                     Palsson, Master. Gosta Leonard   \n",
       "8         1       3  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)   \n",
       "9         1       2                Nasser, Mrs. Nicholas (Adele Achem)   \n",
       "\n",
       "      sex   age  sibsp  parch            ticket     fare cabin embarked  \n",
       "0    male  22.0      1      0         A/5 21171   7.2500   NaN        S  \n",
       "1  female  38.0      1      0          PC 17599  71.2833   C85        C  \n",
       "2  female  26.0      0      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3  female  35.0      1      0            113803  53.1000  C123        S  \n",
       "4    male  35.0      0      0            373450   8.0500   NaN        S  \n",
       "5    male   NaN      0      0            330877   8.4583   NaN        Q  \n",
       "6    male  54.0      0      0             17463  51.8625   E46        S  \n",
       "7    male   2.0      3      1            349909  21.0750   NaN        S  \n",
       "8  female  27.0      0      2            347742  11.1333   NaN        S  \n",
       "9  female  14.0      1      0            237736  30.0708   NaN        C  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the data\n",
    "titanic = pd.read_csv('./data/titanic.csv')\n",
    "titanic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "2d34b605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "survived      0\n",
       "pclass        0\n",
       "name          0\n",
       "sex           0\n",
       "age         177\n",
       "sibsp         0\n",
       "parch         0\n",
       "ticket        0\n",
       "fare          0\n",
       "cabin       687\n",
       "embarked      2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look for missing values\n",
    "titanic.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c64557f",
   "metadata": {},
   "source": [
    "Let's choose our response and a few features, and decide whether we need to adjust them:\n",
    "\n",
    "- **survived:** This is our response, and is already encoded as 0=died and 1=survived.\n",
    "- **pclass:** These are the passenger class categories (1=first class, 2=second class, 3=third class). They are ordered, so we'll leave them as-is.\n",
    "- **sex:** This is a binary category, so we should encode as 0=female and 1=male.\n",
    "- **age:** We need to fill in the missing values.\n",
    "- **embarked:** This is the port they emarked from. There are three unordered categories, so we'll create dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3ecb0396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>1</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>0</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>0</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>1</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>1</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>1</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master. Gosta Leonard</td>\n",
       "      <td>1</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>0</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>0</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>237736</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass                                               name  sex  \\\n",
       "0         0       3                            Braund, Mr. Owen Harris    1   \n",
       "1         1       1  Cumings, Mrs. John Bradley (Florence Briggs Th...    0   \n",
       "2         1       3                             Heikkinen, Miss. Laina    0   \n",
       "3         1       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)    0   \n",
       "4         0       3                           Allen, Mr. William Henry    1   \n",
       "5         0       3                                   Moran, Mr. James    1   \n",
       "6         0       1                            McCarthy, Mr. Timothy J    1   \n",
       "7         0       3                     Palsson, Master. Gosta Leonard    1   \n",
       "8         1       3  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)    0   \n",
       "9         1       2                Nasser, Mrs. Nicholas (Adele Achem)    0   \n",
       "\n",
       "         age  sibsp  parch            ticket     fare cabin embarked  \n",
       "0  22.000000      1      0         A/5 21171   7.2500   NaN        S  \n",
       "1  38.000000      1      0          PC 17599  71.2833   C85        C  \n",
       "2  26.000000      0      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3  35.000000      1      0            113803  53.1000  C123        S  \n",
       "4  35.000000      0      0            373450   8.0500   NaN        S  \n",
       "5  29.699118      0      0            330877   8.4583   NaN        Q  \n",
       "6  54.000000      0      0             17463  51.8625   E46        S  \n",
       "7   2.000000      3      1            349909  21.0750   NaN        S  \n",
       "8  27.000000      0      2            347742  11.1333   NaN        S  \n",
       "9  14.000000      1      0            237736  30.0708   NaN        C  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode sex feature\n",
    "titanic['sex'] = titanic.sex.map({'female':0, 'male':1})\n",
    "\n",
    "# fill in missing values for age\n",
    "titanic.age.fillna(titanic.age.mean(), inplace=True)\n",
    "\n",
    "# print the updated DataFrame\n",
    "titanic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8005d13b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embarked_C</th>\n",
       "      <th>embarked_Q</th>\n",
       "      <th>embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   embarked_C  embarked_Q  embarked_S\n",
       "0           0           0           1\n",
       "1           1           0           0\n",
       "2           0           0           1\n",
       "3           0           0           1\n",
       "4           0           0           1\n",
       "5           0           1           0\n",
       "6           0           0           1\n",
       "7           0           0           1\n",
       "8           0           0           1\n",
       "9           1           0           0"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create three dummy variables using get_dummies\n",
    "pd.get_dummies(titanic.embarked, prefix='embarked').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "089d7ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "      <th>embarked_Q</th>\n",
       "      <th>embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>1</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>0</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>0</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>0</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>1</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>1</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>330877</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>McCarthy, Mr. Timothy J</td>\n",
       "      <td>1</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17463</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>E46</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Palsson, Master. Gosta Leonard</td>\n",
       "      <td>1</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349909</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)</td>\n",
       "      <td>0</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>347742</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Nasser, Mrs. Nicholas (Adele Achem)</td>\n",
       "      <td>0</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>237736</td>\n",
       "      <td>30.0708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass                                               name  sex  \\\n",
       "0         0       3                            Braund, Mr. Owen Harris    1   \n",
       "1         1       1  Cumings, Mrs. John Bradley (Florence Briggs Th...    0   \n",
       "2         1       3                             Heikkinen, Miss. Laina    0   \n",
       "3         1       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)    0   \n",
       "4         0       3                           Allen, Mr. William Henry    1   \n",
       "5         0       3                                   Moran, Mr. James    1   \n",
       "6         0       1                            McCarthy, Mr. Timothy J    1   \n",
       "7         0       3                     Palsson, Master. Gosta Leonard    1   \n",
       "8         1       3  Johnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)    0   \n",
       "9         1       2                Nasser, Mrs. Nicholas (Adele Achem)    0   \n",
       "\n",
       "         age  sibsp  parch            ticket     fare cabin embarked  \\\n",
       "0  22.000000      1      0         A/5 21171   7.2500   NaN        S   \n",
       "1  38.000000      1      0          PC 17599  71.2833   C85        C   \n",
       "2  26.000000      0      0  STON/O2. 3101282   7.9250   NaN        S   \n",
       "3  35.000000      1      0            113803  53.1000  C123        S   \n",
       "4  35.000000      0      0            373450   8.0500   NaN        S   \n",
       "5  29.699118      0      0            330877   8.4583   NaN        Q   \n",
       "6  54.000000      0      0             17463  51.8625   E46        S   \n",
       "7   2.000000      3      1            349909  21.0750   NaN        S   \n",
       "8  27.000000      0      2            347742  11.1333   NaN        S   \n",
       "9  14.000000      1      0            237736  30.0708   NaN        C   \n",
       "\n",
       "   embarked_Q  embarked_S  \n",
       "0           0           1  \n",
       "1           0           0  \n",
       "2           0           1  \n",
       "3           0           1  \n",
       "4           0           1  \n",
       "5           1           0  \n",
       "6           0           1  \n",
       "7           0           1  \n",
       "8           0           1  \n",
       "9           0           0  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create three dummy variables, drop the first dummy variable, and store this as a DataFrame\n",
    "embarked_dummies = pd.get_dummies(titanic.embarked, prefix='embarked').iloc[:, 1:]\n",
    "\n",
    "# concatenate the two dummy variable columns onto the original DataFrame\n",
    "# note: axis=0 means rows, axis=1 means columns\n",
    "titanic = pd.concat([titanic, embarked_dummies], axis=1)\n",
    "\n",
    "# print the updated DataFrame\n",
    "titanic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0155ae38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of feature columns\n",
    "feature_cols = ['pclass', 'sex', 'age', 'embarked_Q', 'embarked_S']\n",
    "\n",
    "# define X and y\n",
    "X = titanic[feature_cols]\n",
    "y = titanic.survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2ae94d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=1, splitter='best')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit a classification tree with max_depth=3 on all data\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "treeclf = DecisionTreeClassifier(max_depth=3, random_state=1)\n",
    "treeclf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "cf73effc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Graphviz file\n",
    "#with open(\"15_titanic.dot\", 'wb') as f:\n",
    "#    f = export_graphviz(treeclf, out_file=f, feature_names=feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c222ca5b",
   "metadata": {},
   "source": [
    "<img src=\"./img/15_titanic.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da553f01",
   "metadata": {},
   "source": [
    "Notice the split in the bottom right, which was made only to increase node purity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8d28bdde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pclass</td>\n",
       "      <td>0.242664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sex</td>\n",
       "      <td>0.655584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>age</td>\n",
       "      <td>0.064494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>embarked_Q</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>embarked_S</td>\n",
       "      <td>0.037258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature  importance\n",
       "0      pclass    0.242664\n",
       "1         sex    0.655584\n",
       "2         age    0.064494\n",
       "3  embarked_Q    0.000000\n",
       "4  embarked_S    0.037258"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the feature importances\n",
    "pd.DataFrame({'feature':feature_cols, 'importance':treeclf.feature_importances_})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895fcd40",
   "metadata": {},
   "source": [
    "## Wrapping up decision trees\n",
    "\n",
    "Here are some advantages and disadvantages of decision trees that we haven't yet talked about:\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Can be specified as a series of rules, and are thought to more closely approximate human decision-making than other models\n",
    "- Non-parametric (will do better than linear regression if relationship between predictors and response is highly non-linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfc47ef",
   "metadata": {},
   "source": [
    "<img src=\"./img/15_linear_vs_tree.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4c862a",
   "metadata": {},
   "source": [
    "**Disadvantages:**\n",
    "\n",
    "- Small variations in the data can result in a completely different tree\n",
    "- Recursive binary splitting makes \"locally optimal\" decisions that may not result in a globally optimal tree\n",
    "- Can create biased trees if the classes are highly imbalanced\n",
    "\n",
    "Note that there is not just one decision tree algorithm; instead, there are many variations. A few common decision tree algorithms that are often referred to by name are C4.5, C5.0, and CART. (More details are available in the [scikit-learn documentation](http://scikit-learn.org/stable/modules/tree.html#tree-algorithms-id3-c4-5-c5-0-and-cart).) scikit-learn uses an \"optimized version\" of CART."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00f23fc",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- scikit-learn documentation: [Decision Trees](http://scikit-learn.org/stable/modules/tree.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_metadata": {
   "author": "Andreas C. M\\\"ller",
   "title": "Machine Learning with Python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
